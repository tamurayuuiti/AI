<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>リアルタイム顔認識システム</title>
    <!-- モデルの読み込みURLは変更せずそのまま保持しています -->
    <script defer src="libs/tf.min.js"></script>
    <script defer src="libs/face-api.min.js"></script>
    <style>
        /* レスポンシブ対応のCSS調整 */
        body {
            font-family: Arial, sans-serif;
            background-color: #f0f0f0;
            margin: 0;
            padding: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
            height: 100vh;
            color: #333;
        }
        .container {
            text-align: center;
            background-color: #fff;
            padding: 2vh;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
            width: 90vw;
            max-width: 800px;
            box-sizing: border-box;
        }
        h1 {
            color: #333;
            font-size: 5vw;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: auto;
            max-width: 720px;
            border: 2px solid #333;
            border-radius: 8px;
            overflow: hidden;
        }
        video, canvas {
            width: 100%;
            height: auto;
        }
        #status, #statusMode {
            font-size: 4vw;
            color: #007bff;
        }
        #recognizedList {
            margin-top: 2vh;
            font-size: 3vw;
            color: #333;
        }
        #pauseButton {
            margin-top: 2vh;
            padding: 1vh 2vw;
            font-size: 4vw;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>リアルタイム顔認識システム</h1>
        <p>カメラ映像に映った人物を認識します。</p>
        <div class="video-container">
            <video id="video" autoplay muted playsinline></video>
            <canvas id="overlay"></canvas>
        </div>
        <div id="status">準備中...</div>
        <div id="statusMode">モード: 確認中...</div>
        <button id="pauseButton">一時停止</button>
        <div id="recognizedList">現在認識中の顔リスト：</div>
    </div>
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('overlay');
        const ctx = canvas.getContext('2d');
        const statusElement = document.getElementById('status');
        const statusModeElement = document.getElementById('statusMode');
        const pauseButton = document.getElementById('pauseButton');
        const recognizedListElement = document.getElementById('recognizedList');
        const customClasses = ['Daiki_Tode', 'Haruna_Oguro', 'Himeka_Kugai', 'Honoka_Nishikawa', 'Nonoka_Sato', 'Shohei_Otsuka', 'Taiki_Kimura', 'Yuna_Katayose'];
        let recognitionModel = null;
        let isPaused = false;
        let detectedFaces = [];

        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" } });
                video.srcObject = stream;
                return new Promise((resolve) => { video.onloadedmetadata = () => resolve(video); });
            } catch (error) {
                alert("カメラのアクセスが許可されていません。カメラへのアクセスを許可してください。");
                console.error("Error accessing camera:", error);
            }
        }

        async function setupFaceAPI() {
            try {
                // 各モデルファイルの直接URLを指定（変更せず）
                await faceapi.nets.tinyFaceDetector.loadFromUri('https://tamurayuuiti.github.io/AI/models');
                await faceapi.nets.faceLandmark68Net.loadFromUri('https://tamurayuuiti.github.io/AI/models');
                await faceapi.nets.faceRecognitionNet.loadFromUri('https://tamurayuuiti.github.io/AI/models');

                statusElement.textContent = "顔検出モデルの読み込みが完了しました。";
                console.log("顔検出モデルの読み込みが成功しました。");
            } catch (error) {
                console.error("顔検出モデルの読み込みに失敗しました:", error);
            }
        }

        function updateRecognizedList() {
            recognizedListElement.innerHTML = '現在認識中の顔リスト：<br>' + detectedFaces.map(face => `<li>${face}</li>`).join('');
        }

        async function detectFaces() {
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptors();
            ctx.clearRect(0, 0, canvas.width, canvas.height);

            const newDetectedFaces = [];
            detections.forEach((detection, index) => {
                const { x, y, width, height } = detection.detection.box;
                let label = "Unknown";

                if (recognitionModel) {
                    label = recognizeFace(detection.descriptor);
                } else {
                    label = `Unknown${index + 1}`;
                }
                newDetectedFaces.push(label);

                // 枠線とラベルを描画
                ctx.strokeStyle = "green";
                ctx.lineWidth = 2;
                ctx.strokeRect(x, y, width, height);
                ctx.fillStyle = "white";
                ctx.font = "18px Arial";
                ctx.fillText(label, x, y - 10);
            });

            detectedFaces = newDetectedFaces;
            updateRecognizedList();
        }

        function recognizeFace(descriptor) {
            const descriptorTensor = tf.tensor([descriptor]);
            const prediction = recognitionModel.predict(descriptorTensor);
            const labelIndex = prediction.argMax(-1).dataSync()[0];
            return labelIndex < customClasses.length ? customClasses[labelIndex] : 'Unknown';
        }

        pauseButton.addEventListener('click', () => {
            if (isPaused) {
                video.play();
                pauseButton.textContent = "一時停止";
            } else {
                video.pause();
                pauseButton.textContent = "再開";
            }
            isPaused = !isPaused;
        });

        async function main() {
            await setupCamera();
            await setupFaceAPI();
            video.addEventListener('play', () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                setInterval(() => {
                    if (!isPaused) detectFaces();
                }, 100);  // 100msごとに顔検出を実行
            });
        }

        main();
    </script>
</body>
</html>
