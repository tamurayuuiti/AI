<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>リアルタイム顔認識システム</title>
    <script defer src="libs/tf.min.js"></script>
    <script defer src="libs/face-api.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; background-color: #f0f0f0; margin: 0; padding: 0; display: flex; justify-content: center; align-items: center; min-height: 100vh; }
        .container { text-align: center; background-color: #fff; padding: 20px; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); max-width: 90%; width: 800px; }
        h1 { color: #333; font-size: 2em; }
        p { font-size: 1em; color: #666; }
        .video-container { position: relative; width: 100%; padding-top: 75%; margin: 20px 0; border: 2px solid #333; border-radius: 8px; overflow: hidden; }
        video, canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
        #status, #statusMode, #faceList { font-size: 1em; margin-top: 10px; color: #007bff; }
        #pauseButton { margin-top: 10px; padding: 10px 20px; font-size: 1em; cursor: pointer; }
        .face-list-item { font-size: 1em; color: #333; margin-top: 5px; }
        @media (max-width: 768px) { h1 { font-size: 1.5em; } p, #status, #statusMode, #pauseButton { font-size: 0.9em; } }
    </style>
</head>
<body>
    <div class="container">
        <h1>リアルタイム顔認識システム</h1>
        <p>カメラ映像に映った人物を認識します。</p>
        <div class="video-container">
            <video id="video" autoplay muted playsinline></video>
            <canvas id="overlay"></canvas>
        </div>
        <div id="status">準備中...</div>
        <div id="statusMode">モード: 確認中...</div>
        <button id="pauseButton">一時停止</button>
        <div id="faceList"></div>
    </div>
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('overlay');
        const ctx = canvas.getContext('2d');
        const statusElement = document.getElementById('status');
        const statusModeElement = document.getElementById('statusMode');
        const faceListElement = document.getElementById('faceList');
        const pauseButton = document.getElementById('pauseButton');
        let recognitionModel = null;
        let isPaused = false;
        let currentFaces = {};

        async function setupCamera() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" } });
                video.srcObject = stream;
                return new Promise((resolve) => { video.onloadedmetadata = () => resolve(video); });
            } catch (error) {
                alert("カメラのアクセスが許可されていません。カメラへのアクセスを許可してください。");
                console.error("Error accessing camera:", error);
            }
        }

        async function setupFaceAPI() {
            try {
                await faceapi.nets.tinyFaceDetector.loadFromUri('https://tamurayuuiti.github.io/AI/models');
                await faceapi.nets.faceLandmark68Net.loadFromUri('https://tamurayuuiti.github.io/AI/models');
                await faceapi.nets.faceRecognitionNet.loadFromUri('https://tamurayuuiti.github.io/AI/models');
                statusElement.textContent = "顔検出モデルの読み込みが完了しました。";
                console.log("顔検出モデルの読み込みが成功しました。");
            } catch (error) {
                console.error("顔検出モデルの読み込みに失敗しました:", error);
            }
        }

        function updateFaceList(detections) {
            const newFaces = {};
            let unknownCount = 1;
            detections.forEach(detection => {
                const descriptor = detection.descriptor;
                let label = "Unknown";

                for (let face in currentFaces) {
                    if (face === JSON.stringify(descriptor)) {
                        newFaces[face] = currentFaces[face];
                        return;
                    }
                }

                label = `Unknown${unknownCount++}`;
                newFaces[JSON.stringify(descriptor)] = label;
            });
            currentFaces = newFaces;

            faceListElement.innerHTML = '';
            Object.values(currentFaces).forEach(faceLabel => {
                const faceListItem = document.createElement('div');
                faceListItem.className = 'face-list-item';
                faceListItem.textContent = faceLabel;
                faceListElement.appendChild(faceListItem);
            });
        }

        async function detectFaces() {
            const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceDescriptors();
            ctx.clearRect(0, 0, canvas.width, canvas.height);

            if (detections.length > 0) {
                updateFaceList(detections);
                detections.forEach(detection => {
                    const { x, y, width, height } = detection.detection.box;
                    ctx.strokeStyle = "green";
                    ctx.lineWidth = 2;
                    ctx.strokeRect(x, y, width, height);
                    ctx.fillStyle = "white";
                    ctx.font = "18px Arial";
                    ctx.fillText(currentFaces[JSON.stringify(detection.descriptor)], x, y - 10);
                });
            } else {
                currentFaces = {};
                faceListElement.innerHTML = '';
            }
        }

        setInterval(() => {
            if (!isPaused) detectFaces();
        }, 100);

        pauseButton.addEventListener('click', () => {
            isPaused = !isPaused;
            pauseButton.textContent = isPaused ? "再開" : "一時停止";
        });

        async function main() {
            await setupCamera();
            await setupFaceAPI();
            video.addEventListener('play', () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
            });
        }

        main();
    </script>
</body>
</html>
